{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/JoshuaLee0816/diffuserlite.github.io/blob/main/01_DiffuserLite_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# === 1. è¨­å®šä½ çš„ GitHub è³‡è¨Š ===\n",
    "GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')# å»ºè­°ä¹‹å¾Œæ›´æ›\n",
    "GITHUB_USER = \"JoshuaLee0816\"\n",
    "GITHUB_REPO = \"diffuserlite.github.io\"\n",
    "\n",
    "import os\n",
    "os.chdir('/content')\n",
    "\n",
    "# === 2. Clone ä½ çš„å€‰åº«ä½œç‚ºã€Œå”¯ä¸€ã€å·¥ä½œç›®éŒ„ ===\n",
    "!rm -rf /content/{GITHUB_REPO}\n",
    "!git clone https://{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{GITHUB_REPO}.git\n",
    "\n",
    "# é€²å…¥å€‰åº«\n",
    "%cd /content/{GITHUB_REPO}\n",
    "\n",
    "# === 3. å®‰è£ç’°å¢ƒï¼ˆåœ¨ä½ çš„ Repo ç›®éŒ„ä¸‹å®‰è£ï¼‰ ===\n",
    "!pip install -e . -q  # ä»¥å¯ç·¨è¼¯æ¨¡å¼å®‰è£ç›®å‰çš„å€‰åº«\n",
    "!pip install git+https://github.com/Farama-Foundation/D4RL.git --ignore-requires-python -q\n",
    "!pip install \"numpy>=1.26.0,<2.0.0\" -q\n",
    "!pip install --upgrade huggingface_hub -q  # Hugging Face ä¸Šå‚³ç”¨\n",
    "\n",
    "# MuJoCo 210 å®‰è£\n",
    "!mkdir -p /root/.mujoco\n",
    "!wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O /tmp/mujoco210.tar.gz\n",
    "!tar -xzf /tmp/mujoco210.tar.gz -C /root/.mujoco/\n",
    "!apt-get install -qq -y libosmesa6-dev libgl1-mesa-glx libglfw3 patchelf > /dev/null 2>&1\n",
    "\n",
    "# è¨­å®š Git èº«ä»½\n",
    "!git config --global user.email \"310274movie@gmail.com\"\n",
    "!git config --global user.name \"JoshuaLee\"\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒå·²åœ¨ä½ çš„ Repo ä¸­å®‰è£å®Œæˆï¼ç¾åœ¨æ‰€æœ‰è®Šå‹•éƒ½æœƒè¨˜éŒ„åœ¨æ­¤ã€‚\")\n"
   ],
   "metadata": {
    "id": "qsneyQzMxrLu"
   },
   "id": "qsneyQzMxrLu",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# === Hugging Face ç™»å…¥ï¼ˆåªéœ€åŸ·è¡Œä¸€æ¬¡ï¼‰===\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=userdata.get('HF_TOKEN'))"
   ],
   "metadata": {
    "id": "wuu_1zDCi2v1"
   },
   "id": "wuu_1zDCi2v1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478c73d0",
   "metadata": {
    "id": "478c73d0"
   },
   "outputs": [],
   "source": "%%writefile /content/run.py\nimport sys, os, types, json, glob, re, time, traceback\nimport mujoco\nimport numpy as np\nimport yaml\n\n# === å¼·åˆ¶ unbuffered output ===\nsys.stdout = os.fdopen(sys.stdout.fileno(), 'w', buffering=1)\nsys.stderr = os.fdopen(sys.stderr.fileno(), 'w', buffering=1)\n\nprint(\"[DEBUG] run.py é–‹å§‹åŸ·è¡Œ\")\n\nshim = types.ModuleType('mujoco_py')\n\nclass ShimModel:\n    def __init__(self, m):\n        self._m = m\n    @property\n    def actuator_ctrlrange(self):\n        return self._m.actuator_ctrlrange.copy()\n    @property\n    def nq(self): return self._m.nq\n    @property\n    def nv(self): return self._m.nv\n    def __getattr__(self, name):\n        return getattr(self._m, name)\n\nclass ShimData:\n    def __init__(self, d):\n        self._d = d\n    @property\n    def qpos(self): return self._d.qpos\n    @qpos.setter\n    def qpos(self, v): self._d.qpos[:] = v\n    @property\n    def qvel(self): return self._d.qvel\n    @qvel.setter\n    def qvel(self, v): self._d.qvel[:] = v\n    @property\n    def ctrl(self): return self._d.ctrl\n    @ctrl.setter\n    def ctrl(self, v): self._d.ctrl[:] = v\n    def __getattr__(self, name):\n        return getattr(self._d, name)\n\nclass ShimSim:\n    def __init__(self, model):\n        self._m = model._m\n        self._d = mujoco.MjData(self._m)\n        self.model = model\n        self.data = ShimData(self._d)\n    def step(self):\n        mujoco.mj_step(self._m, self._d)\n    def forward(self):\n        mujoco.mj_forward(self._m, self._d)\n    def get_state(self):\n        return type('S',(),{'time':self._d.time,'qpos':self._d.qpos.copy(),'qvel':self._d.qvel.copy(),'act':np.array([]),'udd_state':{}})()\n    def set_state(self, s):\n        self._d.time = s.time\n        self._d.qpos[:] = s.qpos\n        self._d.qvel[:] = s.qvel\n        mujoco.mj_forward(self._m, self._d)\n\nclass MjViewer:\n    def __init__(self, sim): pass\n    def render(self): pass\n\nshim.load_model_from_path = lambda p: ShimModel(mujoco.MjModel.from_xml_path(p))\nshim.MjSim = lambda m: ShimSim(m)\nshim.MjViewer = MjViewer\nshim.MujocoException = Exception\nshim.ignore_mujoco_warnings = type('ctx',(),{'__enter__':lambda s:None,'__exit__':lambda s,*a:None})\nshim.__path__ = []\nfor sub in ['cymj','builder','generated','generated.const']:\n    sys.modules[f'mujoco_py.{sub}'] = types.ModuleType(f'mujoco_py.{sub}')\nsys.modules['mujoco_py'] = shim\n\nprint(\"[DEBUG] mujoco_py shim å·²è¨­å®š\")\n\n# === è¨­å®šè·¯å¾‘ ===\nos.chdir('/content/diffuserlite.github.io')\nos.makedirs('results/figures', exist_ok=True)\n\n# === è®€å–ç’°å¢ƒåç¨± ===\nwith open('configs/diffuserlite/mujoco/mujoco.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\n# è§£æ defaults ä¸­çš„ task\ndefaults = config.get('defaults', [])\nENV_NAME = 'halfcheetah-medium-expert-v2'  # é è¨­å€¼\nfor item in defaults:\n    if isinstance(item, dict) and 'task' in item:\n        ENV_NAME = item['task']\n        break\n\n# ä¹Ÿè®€å– mode\nMODE = config.get('mode', 'training')\n\nprint(f\"ğŸ® è¨“ç·´ç’°å¢ƒ: {ENV_NAME}\")\nprint(f\"ğŸ“‹ è¨“ç·´æ¨¡å¼: {MODE}\")\nprint(f\"ğŸ“ å·¥ä½œç›®éŒ„: {os.getcwd()}\")\nsys.stdout.flush()\n\n# === å¾ HuggingFace ä¸‹è¼‰ä¹‹å‰çš„ loss_log.jsonï¼ˆå¦‚æœå­˜åœ¨ï¼‰===\noriginal_print = print\nloss_log = []\nloss_log_path = f'results/loss_log_{ENV_NAME}.json'\n\ntry:\n    from huggingface_hub import hf_hub_download\n    repo_id = \"JoshuaLee0816/diffuserlite-results\"\n    local_path = hf_hub_download(repo_id=repo_id, filename=f\"loss_log_{ENV_NAME}.json\")\n    with open(local_path, 'r') as f:\n        loss_log = json.load(f)\n    original_print(f\"âœ… å¾ HuggingFace è¼‰å…¥ {len(loss_log)} ç­†æ­·å² loss è¨˜éŒ„\")\n    if loss_log:\n        last_step = loss_log[-1].get('gradient_steps', 0)\n        original_print(f\"   ä¸Šæ¬¡è¨“ç·´åˆ°ç¬¬ {last_step} æ­¥\")\nexcept Exception as e:\n    original_print(f\"âš ï¸ ç„¡æ³•è¼‰å…¥æ­·å² loss_log: {e}\")\n    original_print(\"   å°‡å¾é ­é–‹å§‹è¨˜éŒ„\")\n\nsys.stdout.flush()\n\n# å„²å­˜åˆ°æœ¬åœ°\nwith open(loss_log_path, 'w') as f:\n    json.dump(loss_log, f)\n\n# === HuggingFace ä¸Šå‚³å‡½æ•¸ï¼ˆå¸¶ rate limit è™•ç†ï¼‰===\ndef upload_single_file_with_retry(api, local_path, repo_path, repo_id, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            api.upload_file(\n                path_or_fileobj=local_path,\n                path_in_repo=repo_path,\n                repo_id=repo_id,\n            )\n            return True\n        except Exception as e:\n            error_str = str(e).lower()\n            if \"rate limit\" in error_str or \"429\" in str(e) or \"too many\" in error_str:\n                wait_time = 60 * (attempt + 1)\n                original_print(f\"  â³ Rate limit! ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦ ({attempt+1}/{max_retries})...\")\n                time.sleep(wait_time)\n            else:\n                if attempt < max_retries - 1:\n                    original_print(f\"  âš ï¸ ä¸Šå‚³å¤±æ•—: {e}ï¼Œç­‰å¾… 30 ç§’å¾Œé‡è©¦...\")\n                    time.sleep(30)\n                else:\n                    return False\n    return False\n\ndef upload_to_hf(files_to_upload, message=\"\"):\n    try:\n        from huggingface_hub import HfApi\n        api = HfApi()\n        repo_id = \"JoshuaLee0816/diffuserlite-results\"\n        \n        success_count = 0\n        for local_path, repo_path in files_to_upload:\n            if os.path.exists(local_path):\n                if upload_single_file_with_retry(api, local_path, repo_path, repo_id):\n                    success_count += 1\n                else:\n                    original_print(f\"  âŒ ä¸Šå‚³å¤±æ•—: {repo_path}\")\n                time.sleep(2)\n        \n        if message and success_count > 0:\n            original_print(f\"ğŸ“¤ {message} ({success_count}/{len(files_to_upload)} æª”æ¡ˆ)\")\n    except Exception as e:\n        original_print(f\"âš ï¸ HuggingFace ä¸Šå‚³å¤±æ•—: {e}\")\n\ndef cleanup_old_checkpoints(checkpoint_dir):\n    try:\n        for prefix in ['diffusion0', 'diffusion1', 'diffusion2', 'invdyn']:\n            pattern = os.path.join(checkpoint_dir, f'{prefix}_ckpt_*.pt')\n            files = glob.glob(pattern)\n            numbered_files = [f for f in files if 'latest' not in f]\n            for f in numbered_files:\n                match = re.search(r'_ckpt_(\\d+)\\.pt', f)\n                if match:\n                    step = int(match.group(1))\n                    if step % 5000 != 1000:\n                        os.remove(f)\n    except Exception as e:\n        original_print(f\"âš ï¸ æ¸…ç†èˆŠ checkpoint å¤±æ•—: {e}\")\n\ndef delete_from_hf(files_to_delete):\n    try:\n        from huggingface_hub import HfApi\n        api = HfApi()\n        repo_id = \"JoshuaLee0816/diffuserlite-results\"\n        for repo_path in files_to_delete:\n            try:\n                api.delete_file(path_in_repo=repo_path, repo_id=repo_id)\n                time.sleep(1)\n            except:\n                pass\n    except:\n        pass\n\ndef update_figure():\n    import matplotlib.pyplot as plt\n    if not loss_log:\n        return\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    steps = [d['gradient_steps'] for d in loss_log]\n    for key in ['loss0', 'loss1', 'loss2']:\n        if key in loss_log[0]:\n            values = [d[key] for d in loss_log]\n            axes[0].plot(steps, values, label=key, alpha=0.8)\n    axes[0].set_xlabel('Gradient Steps')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title(f'Diffusion Model Loss ({ENV_NAME})')\n    axes[0].legend()\n    axes[0].grid(True)\n    if 'invdyn_loss' in loss_log[0]:\n        values = [d['invdyn_loss'] for d in loss_log]\n        axes[1].plot(steps, values, color='orange', alpha=0.8)\n        axes[1].set_xlabel('Gradient Steps')\n        axes[1].set_ylabel('Loss')\n        axes[1].set_title(f'Inverse Dynamics Loss ({ENV_NAME})')\n        axes[1].grid(True)\n    plt.tight_layout()\n    plt.savefig(f'results/figures/loss_curve_{ENV_NAME}.png', dpi=150)\n    plt.close()\n\n# === æ””æˆª print ä¾†è¨˜éŒ„ loss ===\nlast_upload_step = 0\ncheckpoint_dir = f'results/diffuserlite_d4rl_mujoco/{ENV_NAME}'\nhf_checkpoint_prefix = f'diffuserlite_d4rl_mujoco/{ENV_NAME}'\n\ndef custom_print(*args, **kwargs):\n    global last_upload_step\n    original_print(*args, **kwargs)\n    sys.stdout.flush()\n    \n    if args and isinstance(args[0], dict) and 'gradient_steps' in args[0]:\n        current_step = args[0]['gradient_steps']\n        loss_log.append(args[0].copy())\n        \n        with open(loss_log_path, 'w') as f:\n            json.dump(loss_log, f)\n        \n        if current_step - last_upload_step >= 1000:\n            update_figure()\n            \n            files = [\n                (loss_log_path, f'loss_log_{ENV_NAME}.json'),\n                (f'results/figures/loss_curve_{ENV_NAME}.png', f'figures/loss_curve_{ENV_NAME}.png'),\n            ]\n            \n            for name in ['diffusion0', 'diffusion1', 'diffusion2', 'invdyn']:\n                ckpt_path = f'{checkpoint_dir}/{name}_ckpt_latest.pt'\n                if os.path.exists(ckpt_path):\n                    files.append((ckpt_path, f'{hf_checkpoint_prefix}/{name}_ckpt_latest.pt'))\n            \n            for name in ['diffusion0', 'diffusion1', 'diffusion2', 'invdyn']:\n                ckpt_path = f'{checkpoint_dir}/{name}_ckpt_{current_step}.pt'\n                if os.path.exists(ckpt_path):\n                    files.append((ckpt_path, f'{hf_checkpoint_prefix}/{name}_ckpt_{current_step}.pt'))\n            \n            upload_to_hf(files, f\"å·²ä¸Šå‚³ step {current_step} çš„ checkpoint\")\n            \n            if current_step % 5000 != 1000 and last_upload_step > 0:\n                files_to_delete = []\n                for name in ['diffusion0', 'diffusion1', 'diffusion2', 'invdyn']:\n                    files_to_delete.append(f'{hf_checkpoint_prefix}/{name}_ckpt_{last_upload_step}.pt')\n                delete_from_hf(files_to_delete)\n            \n            cleanup_old_checkpoints(checkpoint_dir)\n            last_upload_step = current_step\n\nimport builtins\nbuiltins.print = custom_print\n\n# === åœ¨ pipeline ä¸­æ’å…¥ debug è¨Šæ¯ ===\noriginal_print(\"[DEBUG] æº–å‚™ä¿®æ”¹ pipeline åŠ å…¥ debug è¨Šæ¯...\")\n\npipeline_path = 'pipelines/diffuserlite_d4rl_mujoco.py'\nwith open(pipeline_path, 'r') as f:\n    pipeline_code = f.read()\n\n# åœ¨é—œéµä½ç½®æ’å…¥ debug è¨Šæ¯\ndebug_patches = [\n    ('def pipeline(args):', 'def pipeline(args):\\n    print(\"[DEBUG] pipeline() è¢«å‘¼å«\")\\n    print(f\"[DEBUG] args.mode = {args.mode}\")\\n    print(f\"[DEBUG] args.task.env_name = {args.task.env_name}\")\\n    import sys; sys.stdout.flush()'),\n    ('if args.mode == \"training\":', 'if args.mode == \"training\":\\n        print(\"[DEBUG] é€²å…¥ training æ¨¡å¼\")\\n        import sys; sys.stdout.flush()'),\n    ('for batch in loop_dataloader(dataloader):', 'print(\"[DEBUG] é–‹å§‹è¨“ç·´è¿´åœˆ...\")\\n        import sys; sys.stdout.flush()\\n        for batch in loop_dataloader(dataloader):'),\n    ('env = gym.make(args.task.env_name)', 'print(f\"[DEBUG] æ­£åœ¨å»ºç«‹ç’°å¢ƒ: {args.task.env_name}\")\\n    import sys; sys.stdout.flush()\\n    env = gym.make(args.task.env_name)\\n    print(\"[DEBUG] ç’°å¢ƒå»ºç«‹æˆåŠŸ\")'),\n    ('dataloader = DataLoader(', 'print(\"[DEBUG] æ­£åœ¨å»ºç«‹ DataLoader...\")\\n    import sys; sys.stdout.flush()\\n    dataloader = DataLoader('),\n]\n\nfor old, new in debug_patches:\n    if old in pipeline_code:\n        pipeline_code = pipeline_code.replace(old, new, 1)\n        original_print(f\"[DEBUG] å·²æ’å…¥ debug: {old[:30]}...\")\n\n# å„²å­˜ä¿®æ”¹å¾Œçš„ pipeline\nwith open(pipeline_path, 'w') as f:\n    f.write(pipeline_code)\n\noriginal_print(\"[DEBUG] pipeline å·²ä¿®æ”¹å®Œæˆ\")\nsys.stdout.flush()\n\n# === åŸ·è¡Œè¨“ç·´ ===\noriginal_print(\"ğŸš€ é–‹å§‹åŸ·è¡Œè¨“ç·´ pipeline...\")\nsys.stdout.flush()\n\ntry:\n    import runpy\n    runpy.run_path(pipeline_path, run_name='__main__')\nexcept SystemExit as e:\n    if e.code == 0:\n        original_print(\"[DEBUG] pipeline æ­£å¸¸çµæŸ (exit code 0)\")\n    else:\n        original_print(f\"\\nâŒ pipeline ç•°å¸¸çµæŸ (exit code {e.code})\")\nexcept Exception as e:\n    original_print(f\"\\nâŒ è¨“ç·´éç¨‹ç™¼ç”ŸéŒ¯èª¤:\")\n    original_print(traceback.format_exc())\n    sys.exit(1)\n\n# === è¨“ç·´çµæŸå¾Œæœ€çµ‚ä¸Šå‚³ ===\noriginal_print(\"\\n\" + \"=\"*50)\noriginal_print(f\"è¨“ç·´å®Œæˆï¼({ENV_NAME})\")\noriginal_print(\"æ­£åœ¨ä¸Šå‚³æœ€çµ‚çµæœ...\")\n\nupdate_figure()\n\nfiles = [\n    (loss_log_path, f'loss_log_{ENV_NAME}.json'),\n    (f'results/figures/loss_curve_{ENV_NAME}.png', f'figures/loss_curve_{ENV_NAME}.png'),\n]\nfor name in ['diffusion0', 'diffusion1', 'diffusion2', 'invdyn']:\n    ckpt_path = f'{checkpoint_dir}/{name}_ckpt_latest.pt'\n    if os.path.exists(ckpt_path):\n        files.append((ckpt_path, f'{hf_checkpoint_prefix}/{name}_ckpt_latest.pt'))\n\nupload_to_hf(files, \"âœ… æœ€çµ‚çµæœå·²ä¸Šå‚³åˆ° HuggingFace\")\noriginal_print(\"=\"*50)"
  },
  {
   "cell_type": "markdown",
   "id": "n8uivn9znm",
   "source": "# ============================================================\n# æ–·ç·šæ¢å¾©ï¼šå¾ HuggingFace ä¸‹è¼‰ checkpoint\n# ============================================================\n# å¦‚æœ Colab æ–·ç·šï¼Œé‡æ–°åŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼š\n# 1. åŸ·è¡Œã€Œç’°å¢ƒå®‰è£ã€cell\n# 2. åŸ·è¡Œã€ŒHuggingFace ç™»å…¥ã€cell\n# 3. åŸ·è¡Œã€Œå¾ HuggingFace ä¸‹è¼‰ checkpointã€cellï¼ˆä¸‹æ–¹ï¼‰\n# 4. åŸ·è¡Œã€Œè¨­å®šè¨“ç·´åƒæ•¸ã€cell\n# 5. åŸ·è¡Œã€ŒåŸ·è¡Œè¨“ç·´ã€cell",
   "metadata": {
    "id": "n8uivn9znm"
   }
  },
  {
   "cell_type": "code",
   "id": "om82o0w2ff",
   "source": "# === å¾ Hugging Face ä¸‹è¼‰ä¹‹å‰çš„ checkpointï¼ˆæ–·ç·šæ¢å¾©ç”¨ï¼‰===\nfrom huggingface_hub import hf_hub_download, list_repo_files\nimport os\nimport shutil\nimport json\n\nrepo_id = \"JoshuaLee0816/diffuserlite-results\"\nlocal_results_dir = '/content/diffuserlite.github.io/results/diffuserlite_d4rl_mujoco/halfcheetah-medium-expert-v2'\n\n# æª¢æŸ¥ Hugging Face ä¸Šæœ‰æ²’æœ‰ä¹‹å‰çš„ checkpoint\ntry:\n    files = list_repo_files(repo_id)\n    \n    # 1. ä¸‹è¼‰ checkpoint\n    checkpoint_files = [f for f in files if 'halfcheetah-medium-expert-v2' in f and 'latest' in f and f.endswith('.pt')]\n    \n    if checkpoint_files:\n        print(f\"æ‰¾åˆ° {len(checkpoint_files)} å€‹ checkpoint æª”æ¡ˆï¼Œé–‹å§‹ä¸‹è¼‰...\")\n        os.makedirs(local_results_dir, exist_ok=True)\n\n        for f in checkpoint_files:\n            local_path = hf_hub_download(repo_id=repo_id, filename=f)\n            filename = os.path.basename(f)\n            dest_path = os.path.join(local_results_dir, filename)\n            shutil.copy(local_path, dest_path)\n            print(f\"  âœ… {filename}\")\n        print(f\"\\nâœ… å·²å¾ Hugging Face æ¢å¾© checkpointï¼\")\n    else:\n        print(\"âš ï¸ Hugging Face ä¸Šæ²’æœ‰æ‰¾åˆ° checkpointï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´\")\n    \n    # 2. ä¸‹è¼‰ loss_log.jsonï¼ˆç”¨æ–¼æ¥çºŒåœ–è¡¨ï¼‰\n    if 'loss_log.json' in files:\n        local_path = hf_hub_download(repo_id=repo_id, filename=\"loss_log.json\")\n        dest_path = '/content/diffuserlite.github.io/results/loss_log.json'\n        os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n        shutil.copy(local_path, dest_path)\n        \n        with open(dest_path, 'r') as f:\n            loss_log = json.load(f)\n        if loss_log:\n            last_step = loss_log[-1].get('gradient_steps', 0)\n            print(f\"âœ… å·²æ¢å¾© loss_log.jsonï¼ˆ{len(loss_log)} ç­†è¨˜éŒ„ï¼Œä¸Šæ¬¡åˆ°ç¬¬ {last_step} æ­¥ï¼‰\")\n    else:\n        print(\"âš ï¸ æ²’æœ‰æ‰¾åˆ° loss_log.jsonï¼Œå°‡å¾é ­é–‹å§‹è¨˜éŒ„\")\n        \nexcept Exception as e:\n    print(f\"âš ï¸ ç„¡æ³•é€£æ¥ Hugging Face: {e}\")\n    print(\"   å°‡å¾é ­é–‹å§‹è¨“ç·´\")\n\n# é¡¯ç¤ºç›®å‰æœ¬åœ°çš„ checkpoint\nprint(f\"\\n=== æœ¬åœ° checkpoint ç‹€æ…‹ ===\")\nif os.path.exists(local_results_dir):\n    files = os.listdir(local_results_dir)\n    latest_files = [f for f in files if 'latest' in f]\n    if latest_files:\n        for f in sorted(latest_files):\n            size_mb = os.path.getsize(os.path.join(local_results_dir, f)) / (1024*1024)\n            print(f\"  {f}: {size_mb:.1f} MB\")\n    else:\n        print(\"  (ç„¡ checkpoint)\")\nelse:\n    print(\"  (ç›®éŒ„ä¸å­˜åœ¨)\")",
   "metadata": {
    "id": "om82o0w2ff"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === éšæ®µ 1ï¼šè¨­å®šæ­£å¼è¨“ç·´åƒæ•¸ ===\n# è¨“ç·´ 100,000 æ­¥ï¼Œæ¯ 1000 æ­¥å­˜ checkpoint\n\n!sed -i 's/save_interval: [0-9]*/save_interval: 1000/' /content/diffuserlite.github.io/configs/diffuserlite/mujoco/mujoco.yaml\n!sed -i 's/diffusion_gradient_steps: [0-9]*/diffusion_gradient_steps: 100000/' /content/diffuserlite.github.io/configs/diffuserlite/mujoco/mujoco.yaml\n!sed -i 's/invdyn_gradient_steps: [0-9]*/invdyn_gradient_steps: 100000/' /content/diffuserlite.github.io/configs/diffuserlite/mujoco/mujoco.yaml\n!sed -i 's/log_interval: [0-9]*/log_interval: 100/' /content/diffuserlite.github.io/configs/diffuserlite/mujoco/mujoco.yaml\n\n# ç¢ºèªä¿®æ”¹çµæœ\n!echo \"=== æ­£å¼è¨“ç·´åƒæ•¸ï¼ˆ100K æ­¥ï¼‰===\"\n!grep -E \"save_interval|diffusion_gradient_steps|invdyn_gradient_steps|log_interval|device:\" /content/diffuserlite.github.io/configs/diffuserlite/mujoco/mujoco.yaml",
   "metadata": {
    "id": "CtmBq_jJddyV"
   },
   "id": "CtmBq_jJddyV",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fj14teps15v",
   "source": "# ============================================================\n# ç’°å¢ƒé¸æ“‡ï¼ˆä¸‰é¸ä¸€åŸ·è¡Œï¼‰\n# ============================================================\n# é è¨­æ˜¯ halfcheetahï¼Œå¦‚æœè¦è·‘å…¶ä»–ç’°å¢ƒï¼ŒåŸ·è¡Œå°æ‡‰çš„ cell\n# \n# | ç’°å¢ƒ | è«–æ–‡ R1 åˆ†æ•¸ |\n# |------|-------------|\n# | halfcheetah-medium-expert-v2 | 91.9 |\n# | walker2d-medium-expert-v2 | 109.1 |\n# | hopper-medium-expert-v2 | 103.3 |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "et1yguepctk",
   "source": "# === é¸æ“‡ç’°å¢ƒï¼šHalfCheetahï¼ˆé è¨­ï¼‰===\nENV_NAME = \"halfcheetah-medium-expert-v2\"\n\nimport subprocess\nsubprocess.run(['sed', '-i', f's/task: .*/task: {ENV_NAME}/', '/content/diffuserlite.github.io/configs/diffuserlite/mujoco/mujoco.yaml'])\nprint(f\"âœ… å·²è¨­å®šç’°å¢ƒ: {ENV_NAME}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ih9g1rbolw",
   "source": "# === é¸æ“‡ç’°å¢ƒï¼šWalker2d ===\nENV_NAME = \"walker2d-medium-expert-v2\"\n\nimport subprocess\nsubprocess.run(['sed', '-i', f's/task: .*/task: {ENV_NAME}/', '/content/diffuserlite.github.io/configs/diffuserlite/mujoco/mujoco.yaml'])\nprint(f\"âœ… å·²è¨­å®šç’°å¢ƒ: {ENV_NAME}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2po45b0fn0x",
   "source": "# === é¸æ“‡ç’°å¢ƒï¼šHopper ===\nENV_NAME = \"hopper-medium-expert-v2\"\n\nimport subprocess\nsubprocess.run(['sed', '-i', f's/task: .*/task: {ENV_NAME}/', '/content/diffuserlite.github.io/configs/diffuserlite/mujoco/mujoco.yaml'])\nprint(f\"âœ… å·²è¨­å®šç’°å¢ƒ: {ENV_NAME}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === åŸ·è¡Œè¨“ç·´ï¼ˆ100K æ­¥ï¼‰===\n# æœƒè‡ªå‹•ï¼š\n# 1. å¾ HuggingFace ä¸‹è¼‰ä¹‹å‰çš„ loss_log.jsonï¼ˆæ¥çºŒåœ–è¡¨ï¼‰\n# 2. æ¯ 1000 æ­¥ä¸Šå‚³ checkpoint å’Œåœ–è¡¨åˆ° HuggingFace\n# 3. åªä¿ç•™ 1000, 6000, 11000, 16000... æ­¥çš„ checkpointï¼ˆé–“éš” 5000 æ­¥ï¼‰\n#    é€™æ¨£å¯ä»¥åœ¨æ¨è«–æ™‚çœ‹åˆ°å­¸ç¿’éç¨‹\n\n!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco210/bin && python3 /content/run.py",
   "metadata": {
    "id": "qNegtm29aNAn"
   },
   "id": "qNegtm29aNAn",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "j951k572nng",
   "source": "# === ç¢ºèª checkpoint ç‹€æ…‹ï¼ˆå¯é¸ï¼‰===\nimport os\nimport json\n\ncheckpoint_dir = '/content/diffuserlite.github.io/results/diffuserlite_d4rl_mujoco/halfcheetah-medium-expert-v2'\nloss_log_path = '/content/diffuserlite.github.io/results/loss_log.json'\n\nprint(\"=== Checkpoint ç‹€æ…‹ ===\")\nif os.path.exists(checkpoint_dir):\n    files = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith('.pt')])\n    for f in files:\n        filepath = os.path.join(checkpoint_dir, f)\n        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n        print(f\"  {f}: {size_mb:.1f} MB\")\nelse:\n    print(\"  (ç„¡ checkpoint)\")\n\nprint(\"\\n=== Loss Log ç‹€æ…‹ ===\")\nif os.path.exists(loss_log_path):\n    with open(loss_log_path, 'r') as f:\n        loss_log = json.load(f)\n    if loss_log:\n        print(f\"  å…± {len(loss_log)} ç­†è¨˜éŒ„\")\n        print(f\"  æœ€æ–°æ­¥æ•¸: {loss_log[-1].get('gradient_steps', 0)}\")\n    else:\n        print(\"  (ç©ºçš„)\")\nelse:\n    print(\"  (ä¸å­˜åœ¨)\")",
   "metadata": {
    "id": "j951k572nng"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3donmbmis98",
   "source": "# === æ‰‹å‹•ä¸Šå‚³åˆ° HuggingFaceï¼ˆå‚™ä»½ç”¨ï¼Œä¸€èˆ¬ä¸éœ€è¦åŸ·è¡Œï¼‰===\n# è¨“ç·´è…³æœ¬å·²è‡ªå‹•æ¯ 1000 æ­¥ä¸Šå‚³ï¼Œé€™è£¡åªæ˜¯å‚™ä»½é¸é …\n\nfrom huggingface_hub import HfApi\nimport os\nimport glob\n\napi = HfApi()\nrepo_id = \"JoshuaLee0816/diffuserlite-results\"\n\nprint(\"æ­£åœ¨æ‰‹å‹•ä¸Šå‚³åˆ° Hugging Face...\")\n\nfiles_to_upload = [\n    ('/content/diffuserlite.github.io/results/loss_log.json', 'loss_log.json'),\n    ('/content/diffuserlite.github.io/results/figures/loss_curve.png', 'figures/loss_curve.png'),\n]\n\ncheckpoint_dir = '/content/diffuserlite.github.io/results/diffuserlite_d4rl_mujoco/halfcheetah-medium-expert-v2'\nfor name in ['diffusion0', 'diffusion1', 'diffusion2', 'invdyn']:\n    ckpt_path = f'{checkpoint_dir}/{name}_ckpt_latest.pt'\n    if os.path.exists(ckpt_path):\n        files_to_upload.append((ckpt_path, f'diffuserlite_d4rl_mujoco/halfcheetah-medium-expert-v2/{name}_ckpt_latest.pt'))\n\nfor local_path, repo_path in files_to_upload:\n    if os.path.exists(local_path):\n        try:\n            api.upload_file(\n                path_or_fileobj=local_path,\n                path_in_repo=repo_path,\n                repo_id=repo_id,\n            )\n            print(f\"  âœ… {repo_path}\")\n        except Exception as e:\n            print(f\"  âŒ {repo_path}: {e}\")\n\nprint(f\"\\nâœ… ä¸Šå‚³å®Œæˆï¼\")\nprint(f\"ğŸ”— https://huggingface.co/{repo_id}\")",
   "metadata": {
    "id": "3donmbmis98"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}