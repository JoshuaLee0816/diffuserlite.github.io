{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/JoshuaLee0816/diffuserlite.github.io/blob/main/01_DiffuserLite_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# === 1. è¨­å®šä½ çš„ GitHub è³‡è¨Š ===\n",
    "GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')# å»ºè­°ä¹‹å¾Œæ›´æ›\n",
    "GITHUB_USER = \"JoshuaLee0816\"\n",
    "GITHUB_REPO = \"diffuserlite.github.io\"\n",
    "\n",
    "import os\n",
    "os.chdir('/content')\n",
    "\n",
    "# === 2. Clone ä½ çš„å€‰åº«ä½œç‚ºã€Œå”¯ä¸€ã€å·¥ä½œç›®éŒ„ ===\n",
    "!rm -rf /content/{GITHUB_REPO}\n",
    "!git clone https://{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{GITHUB_REPO}.git\n",
    "\n",
    "# é€²å…¥å€‰åº«\n",
    "%cd /content/{GITHUB_REPO}\n",
    "\n",
    "# === 3. å®‰è£ç’°å¢ƒï¼ˆåœ¨ä½ çš„ Repo ç›®éŒ„ä¸‹å®‰è£ï¼‰ ===\n",
    "!pip install -e . -q  # ä»¥å¯ç·¨è¼¯æ¨¡å¼å®‰è£ç›®å‰çš„å€‰åº«\n",
    "!pip install git+https://github.com/Farama-Foundation/D4RL.git --ignore-requires-python -q\n",
    "!pip install \"numpy>=1.26.0,<2.0.0\" -q\n",
    "!pip install --upgrade huggingface_hub -q  # Hugging Face ä¸Šå‚³ç”¨\n",
    "\n",
    "# MuJoCo 210 å®‰è£\n",
    "!mkdir -p /root/.mujoco\n",
    "!wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O /tmp/mujoco210.tar.gz\n",
    "!tar -xzf /tmp/mujoco210.tar.gz -C /root/.mujoco/\n",
    "!apt-get install -qq -y libosmesa6-dev libgl1-mesa-glx libglfw3 patchelf > /dev/null 2>&1\n",
    "\n",
    "# è¨­å®š Git èº«ä»½\n",
    "!git config --global user.email \"310274movie@gmail.com\"\n",
    "!git config --global user.name \"JoshuaLee\"\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒå·²åœ¨ä½ çš„ Repo ä¸­å®‰è£å®Œæˆï¼ç¾åœ¨æ‰€æœ‰è®Šå‹•éƒ½æœƒè¨˜éŒ„åœ¨æ­¤ã€‚\")\n"
   ],
   "metadata": {
    "id": "qsneyQzMxrLu"
   },
   "id": "qsneyQzMxrLu",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# === Hugging Face ç™»å…¥ï¼ˆåªéœ€åŸ·è¡Œä¸€æ¬¡ï¼‰===\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=userdata.get('HF_TOKEN'))"
   ],
   "metadata": {
    "id": "wuu_1zDCi2v1"
   },
   "id": "wuu_1zDCi2v1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478c73d0",
   "metadata": {
    "id": "478c73d0"
   },
   "outputs": [],
   "source": "%%writefile /content/run.py\nimport sys, os, types, json, glob, re, time\nimport mujoco\nimport numpy as np\n\nshim = types.ModuleType('mujoco_py')\n\nclass ShimModel:\n    def __init__(self, m):\n        self._m = m\n    @property\n    def actuator_ctrlrange(self):\n        return self._m.actuator_ctrlrange.copy()\n    @property\n    def nq(self): return self._m.nq\n    @property\n    def nv(self): return self._m.nv\n    def __getattr__(self, name):\n        return getattr(self._m, name)\n\nclass ShimData:\n    def __init__(self, d):\n        self._d = d\n    @property\n    def qpos(self): return self._d.qpos\n    @qpos.setter\n    def qpos(self, v): self._d.qpos[:] = v\n    @property\n    def qvel(self): return self._d.qvel\n    @qvel.setter\n    def qvel(self, v): self._d.qvel[:] = v\n    @property\n    def ctrl(self): return self._d.ctrl\n    @ctrl.setter\n    def ctrl(self, v): self._d.ctrl[:] = v\n    def __getattr__(self, name):\n        return getattr(self._d, name)\n\nclass ShimSim:\n    def __init__(self, model):\n        self._m = model._m\n        self._d = mujoco.MjData(self._m)\n        self.model = model\n        self.data = ShimData(self._d)\n    def step(self):\n        mujoco.mj_step(self._m, self._d)\n    def forward(self):\n        mujoco.mj_forward(self._m, self._d)\n    def get_state(self):\n        return type('S',(),{'time':self._d.time,'qpos':self._d.qpos.copy(),'qvel':self._d.qvel.copy(),'act':np.array([]),'udd_state':{}})()\n    def set_state(self, s):\n        self._d.time = s.time\n        self._d.qpos[:] = s.qpos\n        self._d.qvel[:] = s.qvel\n        mujoco.mj_forward(self._m, self._d)\n\nclass MjViewer:\n    def __init__(self, sim): pass\n    def render(self): pass\n\nshim.load_model_from_path = lambda p: ShimModel(mujoco.MjModel.from_xml_path(p))\nshim.MjSim = lambda m: ShimSim(m)\nshim.MjViewer = MjViewer\nshim.MujocoException = Exception\nshim.ignore_mujoco_warnings = type('ctx',(),{'__enter__':lambda s:None,'__exit__':lambda s,*a:None})\nshim.__path__ = []\nfor sub in ['cymj','builder','generated','generated.const']:\n    sys.modules[f'mujoco_py.{sub}'] = types.ModuleType(f'mujoco_py.{sub}')\nsys.modules['mujoco_py'] = shim\n\n# === è¨­å®šè·¯å¾‘ ===\nos.chdir('/content/diffuserlite.github.io')\nos.makedirs('results/figures', exist_ok=True)\n\n# === å¾ HuggingFace ä¸‹è¼‰ä¹‹å‰çš„ loss_log.jsonï¼ˆå¦‚æœå­˜åœ¨ï¼‰===\noriginal_print = print\nloss_log = []\nloss_log_path = 'results/loss_log.json'\n\ntry:\n    from huggingface_hub import hf_hub_download\n    repo_id = \"JoshuaLee0816/diffuserlite-results\"\n    local_path = hf_hub_download(repo_id=repo_id, filename=\"loss_log.json\")\n    with open(local_path, 'r') as f:\n        loss_log = json.load(f)\n    original_print(f\"âœ… å¾ HuggingFace è¼‰å…¥ {len(loss_log)} ç­†æ­·å² loss è¨˜éŒ„\")\n    # æ‰¾å‡ºä¸Šæ¬¡è¨“ç·´åˆ°å“ªä¸€æ­¥\n    if loss_log:\n        last_step = loss_log[-1].get('gradient_steps', 0)\n        original_print(f\"   ä¸Šæ¬¡è¨“ç·´åˆ°ç¬¬ {last_step} æ­¥\")\nexcept Exception as e:\n    original_print(f\"âš ï¸ ç„¡æ³•è¼‰å…¥æ­·å² loss_log: {e}\")\n    original_print(\"   å°‡å¾é ­é–‹å§‹è¨˜éŒ„\")\n\n# å„²å­˜åˆ°æœ¬åœ°\nwith open(loss_log_path, 'w') as f:\n    json.dump(loss_log, f)\n\n# === HuggingFace ä¸Šå‚³å‡½æ•¸ï¼ˆå¸¶ rate limit è™•ç†ï¼‰===\ndef upload_single_file_with_retry(api, local_path, repo_path, repo_id, max_retries=5):\n    \"\"\"ä¸Šå‚³å–®å€‹æª”æ¡ˆï¼Œå¸¶é‡è©¦æ©Ÿåˆ¶\"\"\"\n    for attempt in range(max_retries):\n        try:\n            api.upload_file(\n                path_or_fileobj=local_path,\n                path_in_repo=repo_path,\n                repo_id=repo_id,\n            )\n            return True\n        except Exception as e:\n            error_str = str(e).lower()\n            if \"rate limit\" in error_str or \"429\" in str(e) or \"too many\" in error_str:\n                wait_time = 60 * (attempt + 1)  # 60, 120, 180, 240, 300 ç§’\n                original_print(f\"  â³ Rate limit! ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦ ({attempt+1}/{max_retries})...\")\n                time.sleep(wait_time)\n            else:\n                if attempt < max_retries - 1:\n                    original_print(f\"  âš ï¸ ä¸Šå‚³å¤±æ•—: {e}ï¼Œç­‰å¾… 30 ç§’å¾Œé‡è©¦...\")\n                    time.sleep(30)\n                else:\n                    return False\n    return False\n\ndef upload_to_hf(files_to_upload, message=\"\"):\n    \"\"\"ä¸Šå‚³æŒ‡å®šæª”æ¡ˆåˆ° HuggingFaceï¼ˆå¸¶ rate limit è™•ç†ï¼‰\"\"\"\n    try:\n        from huggingface_hub import HfApi\n        api = HfApi()\n        repo_id = \"JoshuaLee0816/diffuserlite-results\"\n        \n        success_count = 0\n        for local_path, repo_path in files_to_upload:\n            if os.path.exists(local_path):\n                if upload_single_file_with_retry(api, local_path, repo_path, repo_id):\n                    success_count += 1\n                else:\n                    original_print(f\"  âŒ ä¸Šå‚³å¤±æ•—: {repo_path}\")\n                # æ¯å€‹æª”æ¡ˆé–“éš” 2 ç§’ï¼Œé¿å… rate limit\n                time.sleep(2)\n        \n        if message and success_count > 0:\n            original_print(f\"ğŸ“¤ {message} ({success_count}/{len(files_to_upload)} æª”æ¡ˆ)\")\n    except Exception as e:\n        original_print(f\"âš ï¸ HuggingFace ä¸Šå‚³å¤±æ•—: {e}\")\n\ndef cleanup_old_checkpoints(checkpoint_dir):\n    \"\"\"åˆªé™¤èˆŠçš„ checkpointï¼Œåªä¿ç•™ 1000, 6000, 11000, 16000... æ­¥çš„ checkpoint\"\"\"\n    try:\n        for prefix in ['diffusion0', 'diffusion1', 'diffusion2', 'invdyn']:\n            pattern = os.path.join(checkpoint_dir, f'{prefix}_ckpt_*.pt')\n            files = glob.glob(pattern)\n            # éæ¿¾æ‰ latest\n            numbered_files = [f for f in files if 'latest' not in f]\n\n            for f in numbered_files:\n                match = re.search(r'_ckpt_(\\d+)\\.pt', f)\n                if match:\n                    step = int(match.group(1))\n                    # åªä¿ç•™ step % 5000 == 1000 çš„ï¼ˆ1000, 6000, 11000, 16000...ï¼‰\n                    if step % 5000 != 1000:\n                        os.remove(f)\n    except Exception as e:\n        original_print(f\"âš ï¸ æ¸…ç†èˆŠ checkpoint å¤±æ•—: {e}\")\n\ndef delete_from_hf(files_to_delete):\n    \"\"\"å¾ HuggingFace åˆªé™¤æŒ‡å®šæª”æ¡ˆ\"\"\"\n    try:\n        from huggingface_hub import HfApi\n        api = HfApi()\n        repo_id = \"JoshuaLee0816/diffuserlite-results\"\n        \n        for repo_path in files_to_delete:\n            try:\n                api.delete_file(path_in_repo=repo_path, repo_id=repo_id)\n                time.sleep(1)  # åˆªé™¤é–“éš” 1 ç§’\n            except:\n                pass  # æª”æ¡ˆå¯èƒ½ä¸å­˜åœ¨\n    except Exception as e:\n        pass  # éœé»˜å¤±æ•—\n\ndef update_figure():\n    \"\"\"æ›´æ–° loss æ›²ç·šåœ–\"\"\"\n    import matplotlib.pyplot as plt\n    \n    if not loss_log:\n        return\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    steps = [d['gradient_steps'] for d in loss_log]\n    \n    # Diffusion losses\n    for key in ['loss0', 'loss1', 'loss2']:\n        if key in loss_log[0]:\n            values = [d[key] for d in loss_log]\n            axes[0].plot(steps, values, label=key, alpha=0.8)\n    axes[0].set_xlabel('Gradient Steps')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Diffusion Model Loss')\n    axes[0].legend()\n    axes[0].grid(True)\n    \n    # Invdyn loss\n    if 'invdyn_loss' in loss_log[0]:\n        values = [d['invdyn_loss'] for d in loss_log]\n        axes[1].plot(steps, values, color='orange', alpha=0.8)\n        axes[1].set_xlabel('Gradient Steps')\n        axes[1].set_ylabel('Loss')\n        axes[1].set_title('Inverse Dynamics Loss')\n        axes[1].grid(True)\n    \n    plt.tight_layout()\n    plt.savefig('results/figures/loss_curve.png', dpi=150)\n    plt.close()\n\n# === æ””æˆª print ä¾†è¨˜éŒ„ loss ===\nlast_upload_step = 0\n\ndef custom_print(*args, **kwargs):\n    global last_upload_step\n    original_print(*args, **kwargs)\n    \n    if args and isinstance(args[0], dict) and 'gradient_steps' in args[0]:\n        current_step = args[0]['gradient_steps']\n        loss_log.append(args[0].copy())\n        \n        # å„²å­˜ JSON\n        with open(loss_log_path, 'w') as f:\n            json.dump(loss_log, f)\n        \n        # æ¯ 1000 æ­¥ä¸Šå‚³ä¸€æ¬¡åˆ° HuggingFace\n        if current_step - last_upload_step >= 1000:\n            # æ›´æ–°åœ–ç‰‡\n            update_figure()\n            \n            checkpoint_dir = 'results/diffuserlite_d4rl_mujoco/halfcheetah-medium-expert-v2'\n            \n            # æº–å‚™ä¸Šå‚³æª”æ¡ˆåˆ—è¡¨\n            files = [\n                (loss_log_path, 'loss_log.json'),\n                ('results/figures/loss_curve.png', 'figures/loss_curve.png'),\n            ]\n            \n            # ä¸Šå‚³ latest checkpoint\n            for name in ['diffusion0', 'diffusion1', 'diffusion2', 'invdyn']:\n                ckpt_path = f'{checkpoint_dir}/{name}_ckpt_latest.pt'\n                if os.path.exists(ckpt_path):\n                    files.append((ckpt_path, f'diffuserlite_d4rl_mujoco/halfcheetah-medium-expert-v2/{name}_ckpt_latest.pt'))\n            \n            # ä¸Šå‚³å¸¶æ­¥æ•¸çš„ checkpointï¼ˆç”¨æ–¼ä¿ç•™æ­·å²ï¼‰\n            for name in ['diffusion0', 'diffusion1', 'diffusion2', 'invdyn']:\n                ckpt_path = f'{checkpoint_dir}/{name}_ckpt_{current_step}.pt'\n                if os.path.exists(ckpt_path):\n                    files.append((ckpt_path, f'diffuserlite_d4rl_mujoco/halfcheetah-medium-expert-v2/{name}_ckpt_{current_step}.pt'))\n            \n            upload_to_hf(files, f\"å·²ä¸Šå‚³ step {current_step} çš„ checkpoint\")\n            \n            # å¦‚æœé€™ä¸€æ­¥ä¸æ˜¯è¦ä¿ç•™çš„ï¼ˆstep % 5000 != 1000ï¼‰ï¼Œå¾ HuggingFace åˆªé™¤å‰ä¸€å€‹\n            # ä¿ç•™: 1000, 6000, 11000, 16000...\n            if current_step % 5000 != 1000 and last_upload_step > 0:\n                # åˆªé™¤ä¸Šä¸€å€‹ checkpointï¼ˆå› ç‚ºä¸éœ€è¦ä¿ç•™ï¼‰\n                files_to_delete = []\n                for name in ['diffusion0', 'diffusion1', 'diffusion2', 'invdyn']:\n                    files_to_delete.append(f'diffuserlite_d4rl_mujoco/halfcheetah-medium-expert-v2/{name}_ckpt_{last_upload_step}.pt')\n                delete_from_hf(files_to_delete)\n            \n            # æ¸…ç†æœ¬åœ°èˆŠ checkpoint\n            cleanup_old_checkpoints(checkpoint_dir)\n            \n            last_upload_step = current_step\n\nimport builtins\nbuiltins.print = custom_print\n\n# === åŸ·è¡Œè¨“ç·´ ===\nimport runpy\nrunpy.run_path('pipelines/diffuserlite_d4rl_mujoco.py', run_name='__main__')\n\n# === è¨“ç·´çµæŸå¾Œæœ€çµ‚ä¸Šå‚³ ===\noriginal_print(\"\\n\" + \"=\"*50)\noriginal_print(\"è¨“ç·´å®Œæˆï¼æ­£åœ¨ä¸Šå‚³æœ€çµ‚çµæœ...\")\n\nupdate_figure()\n\ncheckpoint_dir = 'results/diffuserlite_d4rl_mujoco/halfcheetah-medium-expert-v2'\nfiles = [\n    (loss_log_path, 'loss_log.json'),\n    ('results/figures/loss_curve.png', 'figures/loss_curve.png'),\n]\nfor name in ['diffusion0', 'diffusion1', 'diffusion2', 'invdyn']:\n    ckpt_path = f'{checkpoint_dir}/{name}_ckpt_latest.pt'\n    if os.path.exists(ckpt_path):\n        files.append((ckpt_path, f'diffuserlite_d4rl_mujoco/halfcheetah-medium-expert-v2/{name}_ckpt_latest.pt'))\n\nupload_to_hf(files, \"âœ… æœ€çµ‚çµæœå·²ä¸Šå‚³åˆ° HuggingFace\")\noriginal_print(\"=\"*50)"
  },
  {
   "cell_type": "markdown",
   "id": "n8uivn9znm",
   "source": "# ============================================================\n# æ–·ç·šæ¢å¾©ï¼šå¾ HuggingFace ä¸‹è¼‰ checkpoint\n# ============================================================\n# å¦‚æœ Colab æ–·ç·šï¼Œé‡æ–°åŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼š\n# 1. åŸ·è¡Œã€Œç’°å¢ƒå®‰è£ã€cell\n# 2. åŸ·è¡Œã€ŒHuggingFace ç™»å…¥ã€cell\n# 3. åŸ·è¡Œã€Œå¾ HuggingFace ä¸‹è¼‰ checkpointã€cellï¼ˆä¸‹æ–¹ï¼‰\n# 4. åŸ·è¡Œã€Œè¨­å®šè¨“ç·´åƒæ•¸ã€cell\n# 5. åŸ·è¡Œã€ŒåŸ·è¡Œè¨“ç·´ã€cell",
   "metadata": {
    "id": "n8uivn9znm"
   }
  },
  {
   "cell_type": "code",
   "id": "om82o0w2ff",
   "source": "# === å¾ Hugging Face ä¸‹è¼‰ä¹‹å‰çš„ checkpointï¼ˆæ–·ç·šæ¢å¾©ç”¨ï¼‰===\nfrom huggingface_hub import hf_hub_download, list_repo_files\nimport os\nimport shutil\nimport json\n\nrepo_id = \"JoshuaLee0816/diffuserlite-results\"\nlocal_results_dir = '/content/diffuserlite.github.io/results/diffuserlite_d4rl_mujoco/halfcheetah-medium-expert-v2'\n\n# æª¢æŸ¥ Hugging Face ä¸Šæœ‰æ²’æœ‰ä¹‹å‰çš„ checkpoint\ntry:\n    files = list_repo_files(repo_id)\n    \n    # 1. ä¸‹è¼‰ checkpoint\n    checkpoint_files = [f for f in files if 'halfcheetah-medium-expert-v2' in f and 'latest' in f and f.endswith('.pt')]\n    \n    if checkpoint_files:\n        print(f\"æ‰¾åˆ° {len(checkpoint_files)} å€‹ checkpoint æª”æ¡ˆï¼Œé–‹å§‹ä¸‹è¼‰...\")\n        os.makedirs(local_results_dir, exist_ok=True)\n\n        for f in checkpoint_files:\n            local_path = hf_hub_download(repo_id=repo_id, filename=f)\n            filename = os.path.basename(f)\n            dest_path = os.path.join(local_results_dir, filename)\n            shutil.copy(local_path, dest_path)\n            print(f\"  âœ… {filename}\")\n        print(f\"\\nâœ… å·²å¾ Hugging Face æ¢å¾© checkpointï¼\")\n    else:\n        print(\"âš ï¸ Hugging Face ä¸Šæ²’æœ‰æ‰¾åˆ° checkpointï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´\")\n    \n    # 2. ä¸‹è¼‰ loss_log.jsonï¼ˆç”¨æ–¼æ¥çºŒåœ–è¡¨ï¼‰\n    if 'loss_log.json' in files:\n        local_path = hf_hub_download(repo_id=repo_id, filename=\"loss_log.json\")\n        dest_path = '/content/diffuserlite.github.io/results/loss_log.json'\n        os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n        shutil.copy(local_path, dest_path)\n        \n        with open(dest_path, 'r') as f:\n            loss_log = json.load(f)\n        if loss_log:\n            last_step = loss_log[-1].get('gradient_steps', 0)\n            print(f\"âœ… å·²æ¢å¾© loss_log.jsonï¼ˆ{len(loss_log)} ç­†è¨˜éŒ„ï¼Œä¸Šæ¬¡åˆ°ç¬¬ {last_step} æ­¥ï¼‰\")\n    else:\n        print(\"âš ï¸ æ²’æœ‰æ‰¾åˆ° loss_log.jsonï¼Œå°‡å¾é ­é–‹å§‹è¨˜éŒ„\")\n        \nexcept Exception as e:\n    print(f\"âš ï¸ ç„¡æ³•é€£æ¥ Hugging Face: {e}\")\n    print(\"   å°‡å¾é ­é–‹å§‹è¨“ç·´\")\n\n# é¡¯ç¤ºç›®å‰æœ¬åœ°çš„ checkpoint\nprint(f\"\\n=== æœ¬åœ° checkpoint ç‹€æ…‹ ===\")\nif os.path.exists(local_results_dir):\n    files = os.listdir(local_results_dir)\n    latest_files = [f for f in files if 'latest' in f]\n    if latest_files:\n        for f in sorted(latest_files):\n            size_mb = os.path.getsize(os.path.join(local_results_dir, f)) / (1024*1024)\n            print(f\"  {f}: {size_mb:.1f} MB\")\n    else:\n        print(\"  (ç„¡ checkpoint)\")\nelse:\n    print(\"  (ç›®éŒ„ä¸å­˜åœ¨)\")",
   "metadata": {
    "id": "om82o0w2ff"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === éšæ®µ 1ï¼šè¨­å®šæ­£å¼è¨“ç·´åƒæ•¸ ===\n# è¨“ç·´ 100,000 æ­¥ï¼Œæ¯ 1000 æ­¥å­˜ checkpoint\n\n!sed -i 's/save_interval: [0-9]*/save_interval: 1000/' /content/diffuserlite.github.io/configs/diffuserlite/mujoco/mujoco.yaml\n!sed -i 's/diffusion_gradient_steps: [0-9]*/diffusion_gradient_steps: 100000/' /content/diffuserlite.github.io/configs/diffuserlite/mujoco/mujoco.yaml\n!sed -i 's/invdyn_gradient_steps: [0-9]*/invdyn_gradient_steps: 100000/' /content/diffuserlite.github.io/configs/diffuserlite/mujoco/mujoco.yaml\n!sed -i 's/log_interval: [0-9]*/log_interval: 100/' /content/diffuserlite.github.io/configs/diffuserlite/mujoco/mujoco.yaml\n\n# ç¢ºèªä¿®æ”¹çµæœ\n!echo \"=== æ­£å¼è¨“ç·´åƒæ•¸ï¼ˆ100K æ­¥ï¼‰===\"\n!grep -E \"save_interval|diffusion_gradient_steps|invdyn_gradient_steps|log_interval|device:\" /content/diffuserlite.github.io/configs/diffuserlite/mujoco/mujoco.yaml",
   "metadata": {
    "id": "CtmBq_jJddyV"
   },
   "id": "CtmBq_jJddyV",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === åŸ·è¡Œè¨“ç·´ï¼ˆ100K æ­¥ï¼‰===\n# æœƒè‡ªå‹•ï¼š\n# 1. å¾ HuggingFace ä¸‹è¼‰ä¹‹å‰çš„ loss_log.jsonï¼ˆæ¥çºŒåœ–è¡¨ï¼‰\n# 2. æ¯ 1000 æ­¥ä¸Šå‚³ checkpoint å’Œåœ–è¡¨åˆ° HuggingFace\n# 3. åªä¿ç•™ 1000, 6000, 11000, 16000... æ­¥çš„ checkpointï¼ˆé–“éš” 5000 æ­¥ï¼‰\n#    é€™æ¨£å¯ä»¥åœ¨æ¨è«–æ™‚çœ‹åˆ°å­¸ç¿’éç¨‹\n\n!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco210/bin && python3 /content/run.py",
   "metadata": {
    "id": "qNegtm29aNAn"
   },
   "id": "qNegtm29aNAn",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "j951k572nng",
   "source": "# === ç¢ºèª checkpoint ç‹€æ…‹ï¼ˆå¯é¸ï¼‰===\nimport os\nimport json\n\ncheckpoint_dir = '/content/diffuserlite.github.io/results/diffuserlite_d4rl_mujoco/halfcheetah-medium-expert-v2'\nloss_log_path = '/content/diffuserlite.github.io/results/loss_log.json'\n\nprint(\"=== Checkpoint ç‹€æ…‹ ===\")\nif os.path.exists(checkpoint_dir):\n    files = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith('.pt')])\n    for f in files:\n        filepath = os.path.join(checkpoint_dir, f)\n        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n        print(f\"  {f}: {size_mb:.1f} MB\")\nelse:\n    print(\"  (ç„¡ checkpoint)\")\n\nprint(\"\\n=== Loss Log ç‹€æ…‹ ===\")\nif os.path.exists(loss_log_path):\n    with open(loss_log_path, 'r') as f:\n        loss_log = json.load(f)\n    if loss_log:\n        print(f\"  å…± {len(loss_log)} ç­†è¨˜éŒ„\")\n        print(f\"  æœ€æ–°æ­¥æ•¸: {loss_log[-1].get('gradient_steps', 0)}\")\n    else:\n        print(\"  (ç©ºçš„)\")\nelse:\n    print(\"  (ä¸å­˜åœ¨)\")",
   "metadata": {
    "id": "j951k572nng"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3donmbmis98",
   "source": "# === æ‰‹å‹•ä¸Šå‚³åˆ° HuggingFaceï¼ˆå‚™ä»½ç”¨ï¼Œä¸€èˆ¬ä¸éœ€è¦åŸ·è¡Œï¼‰===\n# è¨“ç·´è…³æœ¬å·²è‡ªå‹•æ¯ 1000 æ­¥ä¸Šå‚³ï¼Œé€™è£¡åªæ˜¯å‚™ä»½é¸é …\n\nfrom huggingface_hub import HfApi\nimport os\nimport glob\n\napi = HfApi()\nrepo_id = \"JoshuaLee0816/diffuserlite-results\"\n\nprint(\"æ­£åœ¨æ‰‹å‹•ä¸Šå‚³åˆ° Hugging Face...\")\n\nfiles_to_upload = [\n    ('/content/diffuserlite.github.io/results/loss_log.json', 'loss_log.json'),\n    ('/content/diffuserlite.github.io/results/figures/loss_curve.png', 'figures/loss_curve.png'),\n]\n\ncheckpoint_dir = '/content/diffuserlite.github.io/results/diffuserlite_d4rl_mujoco/halfcheetah-medium-expert-v2'\nfor name in ['diffusion0', 'diffusion1', 'diffusion2', 'invdyn']:\n    ckpt_path = f'{checkpoint_dir}/{name}_ckpt_latest.pt'\n    if os.path.exists(ckpt_path):\n        files_to_upload.append((ckpt_path, f'diffuserlite_d4rl_mujoco/halfcheetah-medium-expert-v2/{name}_ckpt_latest.pt'))\n\nfor local_path, repo_path in files_to_upload:\n    if os.path.exists(local_path):\n        try:\n            api.upload_file(\n                path_or_fileobj=local_path,\n                path_in_repo=repo_path,\n                repo_id=repo_id,\n            )\n            print(f\"  âœ… {repo_path}\")\n        except Exception as e:\n            print(f\"  âŒ {repo_path}: {e}\")\n\nprint(f\"\\nâœ… ä¸Šå‚³å®Œæˆï¼\")\nprint(f\"ğŸ”— https://huggingface.co/{repo_id}\")",
   "metadata": {
    "id": "3donmbmis98"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}